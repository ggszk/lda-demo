import pandas as pd
import numpy as np
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer
import json
from wordcloud import WordCloud
from collections import Counter
import base64
from io import BytesIO
import requests
import os
import platform
import matplotlib.pyplot as plt

def analyze_receipt_data(n_topics=5):
    """ãƒ¬ã‚·ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ã€æ”¯åº—ã”ã¨ã®è²·ã„ç‰©ãƒˆãƒ”ãƒƒã‚¯ã‚’è¦‹ã¤ã‘ã‚‹
    
    Args:
        n_topics (int): åˆ†æã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰
    """
    
    print(f"ğŸ“Š ãƒ¬ã‚·ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™ï¼ˆ{n_topics}ãƒˆãƒ”ãƒƒã‚¯ã§åˆ†æï¼‰...")
    
    # 1. ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    df = pd.read_csv('ãƒ¬ã‚·ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿.csv')
    print(f"âœ… {len(df)}ä»¶ã®ãƒ¬ã‚·ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    
    # 2. å•†å“åã‚’æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
    vectorizer = CountVectorizer(
        token_pattern=r'\S+',  # ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§å•†å“ã‚’åˆ†å‰²
        min_df=2,              # 2å›ä»¥ä¸Šå‡ºç¾ã™ã‚‹å•†å“ã®ã¿ä½¿ç”¨
        max_df=0.8             # å…¨ä½“ã®80%ä»¥ä¸Šã§å‡ºç¾ã™ã‚‹å•†å“ã¯é™¤å¤–
    )
    
    X = vectorizer.fit_transform(df['è³¼å…¥å•†å“'])
    feature_names = vectorizer.get_feature_names_out()
    print(f"âœ… {len(feature_names)}ç¨®é¡ã®å•†å“ã‚’åˆ†æå¯¾è±¡ã«ã—ã¾ã™")
    
    # 3. è²·ã„ç‰©ãƒˆãƒ”ãƒƒã‚¯ã‚’è‡ªå‹•ã§è¦‹ã¤ã‘ã‚‹
    lda = LatentDirichletAllocation(
        n_components=n_topics,
        random_state=42,
        max_iter=20
    )
    
    # å„ãƒ¬ã‚·ãƒ¼ãƒˆãŒã©ã®ãƒˆãƒ”ãƒƒã‚¯ã«å±ã™ã‚‹ã‹ã®ç¢ºç‡ã‚’è¨ˆç®—
    topic_probs = lda.fit_transform(X)
    print(f"âœ… {n_topics}ã¤ã®è²·ã„ç‰©ãƒˆãƒ”ãƒƒã‚¯ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
    
    # 4. å„ãƒˆãƒ”ãƒƒã‚¯ã«ã©ã‚“ãªå•†å“ãŒå«ã¾ã‚Œã‚‹ã‹ã‚’åˆ†æ
    topics_info = []
    for topic_idx, topic in enumerate(lda.components_):
        # å„ãƒˆãƒ”ãƒƒã‚¯ã§é‡è¦ãªå•†å“ãƒˆãƒƒãƒ—5ã‚’å–å¾—
        top_words_idx = topic.argsort()[-5:][::-1]
        top_words = [feature_names[i] for i in top_words_idx]
        topics_info.append({
            'topic_name': f'ãƒˆãƒ”ãƒƒã‚¯{topic_idx + 1}',
            'top_products': top_words
        })
        print(f"  {topics_info[-1]['topic_name']}: {', '.join(top_words)}")
    
    # 5. æ”¯åº—ã”ã¨ã«ãƒˆãƒ”ãƒƒã‚¯ã®å‰²åˆã‚’è¨ˆç®—
    df_with_topics = df.copy()
    df_with_topics['topic_probs'] = list(topic_probs)
    
    store_analysis = {}
    stores = df['æ”¯åº—'].unique()
    
    for store in stores:
        store_data = df_with_topics[df_with_topics['æ”¯åº—'] == store]
        # ã“ã®æ”¯åº—ã®å…¨ãƒ¬ã‚·ãƒ¼ãƒˆã§ã®ãƒˆãƒ”ãƒƒã‚¯ç¢ºç‡ã®å¹³å‡ã‚’è¨ˆç®—
        avg_topic_probs = np.mean([prob for prob in store_data['topic_probs']], axis=0)
        
        store_analysis[store] = {
            'receipt_count': len(store_data),
            'topic_ratios': avg_topic_probs.tolist()
        }
        
        print(f"\nğŸª {store}åº—ã®åˆ†æçµæœ:")
        print(f"  ãƒ¬ã‚·ãƒ¼ãƒˆæ•°: {len(store_data)}ä»¶")
        for i, ratio in enumerate(avg_topic_probs):
            print(f"  {topics_info[i]['topic_name']}: {ratio:.1%}")
    
    # 6. çµæœã‚’è¿”ã™ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã¯ã—ãªã„ï¼‰
    results = {
        'topics': topics_info,
        'stores': store_analysis,
        'summary': {
            'total_receipts': len(df),
            'total_products': len(feature_names),
            'n_topics': n_topics
        }
    }
    
    print(f"\nâœ… åˆ†æå®Œäº†ï¼çµæœã‚’è¿”ã—ã¾ã™")
    return results


def generate_store_wordclouds():
    """æ”¯åº—åˆ¥ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹
    
    Returns:
        dict: å„æ”¯åº—ã®ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒï¼ˆBase64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼‰
    """
    print("ğŸ“Š æ”¯åº—åˆ¥ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆä¸­...")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    df = pd.read_csv('ãƒ¬ã‚·ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿.csv')
    
    # æ”¯åº—ã®é †åºã‚’å›ºå®šï¼ˆã‚°ãƒ©ãƒ•ã¨åŒã˜é †åºï¼‰
    stores = ['ä¸­å¤®åŒº', 'åŒ—åŒº', 'æ±åŒº', 'è¥¿åŒº']
    store_wordclouds = {}
    
    for store in stores:
        # å„æ”¯åº—ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        store_data = df[df['æ”¯åº—'] == store]
        
        # è³¼å…¥å•†å“ã‚’çµåˆã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
        all_products = ' '.join(store_data['è³¼å…¥å•†å“'].astype(str))
        
        # å•†å“ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        products_list = all_products.split()
        product_counts = Counter(products_list)
        
        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ã‚’å–å¾—
        japanese_font_path = get_japanese_font_path()
        
        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
        wordcloud = WordCloud(
            font_path=japanese_font_path,
            width=250,  # ã‚µã‚¤ã‚ºã‚’å°ã•ã
            height=150,
            background_color='white',
            max_words=30,  # å˜èªæ•°ã‚’æ¸›ã‚‰ã™
            colormap='viridis',
            prefer_horizontal=0.9  # æ°´å¹³ãƒ†ã‚­ã‚¹ãƒˆã‚’å„ªå…ˆ
        ).generate_from_frequencies(product_counts)
        
        # ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆæœ€é©åŒ–ï¼‰
        img_buffer = BytesIO()
        # matplotlibã‚’ä½¿ã‚ãšã«Pillowã§ç›´æ¥ä¿å­˜
        wordcloud_image = wordcloud.to_image()
        wordcloud_image.save(img_buffer, format='PNG', optimize=True)
        
        img_buffer.seek(0)
        img_base64 = base64.b64encode(img_buffer.getvalue()).decode('utf-8')
        
        store_wordclouds[store] = {
            'image': f'data:image/png;base64,{img_base64}',
            'product_count': len(product_counts),
            'top_products': list(product_counts.most_common(10))
        }
        
        print(f"âœ… {store}åº—ã®ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆï¼ˆ{len(product_counts)}ç¨®é¡ã®å•†å“ï¼‰")
    
    return store_wordclouds


def download_japanese_font():
    """ã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒç”¨ï¼šGoogle Fontsã‹ã‚‰Noto Sans JPã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    
    Returns:
        str: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚©ãƒ³ãƒˆã®ãƒ‘ã‚¹ã€ã¾ãŸã¯None
    """
    font_path = "/tmp/NotoSansJP-Regular.ttf"
    
    # æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
    if os.path.exists(font_path):
        print(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨: {font_path}")
        return font_path
    
    try:
        print("ğŸ”„ Google Fontsã‹ã‚‰Noto Sans JPã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        # GitHubã®Google Fontsãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        font_url = "https://github.com/google/fonts/raw/main/ofl/notosansjp/NotoSansJP%5Bwght%5D.ttf"
        
        response = requests.get(font_url, timeout=30)
        response.raise_for_status()
        
        # /tmpãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã„å ´åˆã¯ä½œæˆ
        os.makedirs("/tmp", exist_ok=True)
        
        with open(font_path, 'wb') as f:
            f.write(response.content)
        
        print(f"âœ… æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {font_path}")
        return font_path
        
    except Exception as e:
        print(f"âš ï¸ ãƒ•ã‚©ãƒ³ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
        return None


def get_japanese_font_path():
    """ç’°å¢ƒã«å¿œã˜ãŸæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®ãƒ‘ã‚¹ã‚’å–å¾—
    
    Returns:
        str: æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®ãƒ‘ã‚¹ã€ã¾ãŸã¯None
    """
    # 1. ã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒï¼ˆRenderç­‰ï¼‰ç”¨: Google Fontsã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    cloud_font = download_japanese_font()
    if cloud_font and os.path.exists(cloud_font):
        return cloud_font
    
    # 2. ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒç”¨: ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ³ãƒˆã‚’æ¢ã™
    system_fonts = []
    
    if platform.system() == 'Darwin':  # macOS
        system_fonts = [
            '/System/Library/Fonts/Hiragino Sans GB.ttc',
            '/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ ProN W3.ttc',
            '/Library/Fonts/Arial Unicode MS.ttf',
            '/System/Library/Fonts/Apple Gothic.ttf'
        ]
    elif platform.system() == 'Linux':
        system_fonts = [
            '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc',
            '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',
            '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf'
        ]
    elif platform.system() == 'Windows':
        system_fonts = [
            'C:/Windows/Fonts/msgothic.ttc',
            'C:/Windows/Fonts/meiryo.ttc',
            'C:/Windows/Fonts/arial.ttf'
        ]
    
    for font_path in system_fonts:
        if os.path.exists(font_path):
            print(f"âœ… ã‚·ã‚¹ãƒ†ãƒ æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’ç™ºè¦‹: {font_path}")
            return font_path
    
    print("âš ï¸ æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    return None


if __name__ == "__main__":
    import sys
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ãƒˆãƒ”ãƒƒã‚¯æ•°æŒ‡å®šå¯èƒ½
    n_topics = int(sys.argv[1]) if len(sys.argv) > 1 else 6
    print(f"ğŸ¯ {n_topics}ãƒˆãƒ”ãƒƒã‚¯ã§åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™")
    analyze_receipt_data(n_topics)