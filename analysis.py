import pandas as pd
import numpy as np
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer
import json

def analyze_receipt_data(n_topics=5):
    """ãƒ¬ã‚·ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ã€æ”¯åº—ã”ã¨ã®è²·ã„ç‰©ãƒˆãƒ”ãƒƒã‚¯ã‚’è¦‹ã¤ã‘ã‚‹
    
    Args:
        n_topics (int): åˆ†æã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰
    """
    
    print(f"ğŸ“Š ãƒ¬ã‚·ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™ï¼ˆ{n_topics}ãƒˆãƒ”ãƒƒã‚¯ã§åˆ†æï¼‰...")
    
    # 1. ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    df = pd.read_csv('ãƒ¬ã‚·ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿_v7_æ•™è‚²ç”¨.csv')
    print(f"âœ… {len(df)}ä»¶ã®ãƒ¬ã‚·ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    
    # 2. å•†å“åã‚’æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
    vectorizer = CountVectorizer(
        token_pattern=r'\S+',  # ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§å•†å“ã‚’åˆ†å‰²
        min_df=2,              # 2å›ä»¥ä¸Šå‡ºç¾ã™ã‚‹å•†å“ã®ã¿ä½¿ç”¨
        max_df=0.8             # å…¨ä½“ã®80%ä»¥ä¸Šã§å‡ºç¾ã™ã‚‹å•†å“ã¯é™¤å¤–
    )
    
    X = vectorizer.fit_transform(df['è³¼å…¥å•†å“'])
    feature_names = vectorizer.get_feature_names_out()
    print(f"âœ… {len(feature_names)}ç¨®é¡ã®å•†å“ã‚’åˆ†æå¯¾è±¡ã«ã—ã¾ã™")
    
    # 3. è²·ã„ç‰©ãƒˆãƒ”ãƒƒã‚¯ã‚’è‡ªå‹•ã§è¦‹ã¤ã‘ã‚‹
    lda = LatentDirichletAllocation(
        n_components=n_topics,
        random_state=42,
        max_iter=20
    )
    
    # å„ãƒ¬ã‚·ãƒ¼ãƒˆãŒã©ã®ãƒˆãƒ”ãƒƒã‚¯ã«å±ã™ã‚‹ã‹ã®ç¢ºç‡ã‚’è¨ˆç®—
    topic_probs = lda.fit_transform(X)
    print(f"âœ… {n_topics}ã¤ã®è²·ã„ç‰©ãƒˆãƒ”ãƒƒã‚¯ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
    
    # 4. å„ãƒˆãƒ”ãƒƒã‚¯ã«ã©ã‚“ãªå•†å“ãŒå«ã¾ã‚Œã‚‹ã‹ã‚’åˆ†æ
    topics_info = []
    for topic_idx, topic in enumerate(lda.components_):
        # å„ãƒˆãƒ”ãƒƒã‚¯ã§é‡è¦ãªå•†å“ãƒˆãƒƒãƒ—5ã‚’å–å¾—
        top_words_idx = topic.argsort()[-5:][::-1]
        top_words = [feature_names[i] for i in top_words_idx]
        topics_info.append({
            'topic_name': f'ãƒˆãƒ”ãƒƒã‚¯{topic_idx + 1}',
            'top_products': top_words
        })
        print(f"  {topics_info[-1]['topic_name']}: {', '.join(top_words)}")
    
    # 5. æ”¯åº—ã”ã¨ã«ãƒˆãƒ”ãƒƒã‚¯ã®å‰²åˆã‚’è¨ˆç®—
    df_with_topics = df.copy()
    df_with_topics['topic_probs'] = list(topic_probs)
    
    store_analysis = {}
    stores = df['æ”¯åº—'].unique()
    
    for store in stores:
        store_data = df_with_topics[df_with_topics['æ”¯åº—'] == store]
        # ã“ã®æ”¯åº—ã®å…¨ãƒ¬ã‚·ãƒ¼ãƒˆã§ã®ãƒˆãƒ”ãƒƒã‚¯ç¢ºç‡ã®å¹³å‡ã‚’è¨ˆç®—
        avg_topic_probs = np.mean([prob for prob in store_data['topic_probs']], axis=0)
        
        store_analysis[store] = {
            'receipt_count': len(store_data),
            'topic_ratios': avg_topic_probs.tolist()
        }
        
        print(f"\nğŸª {store}åº—ã®åˆ†æçµæœ:")
        print(f"  ãƒ¬ã‚·ãƒ¼ãƒˆæ•°: {len(store_data)}ä»¶")
        for i, ratio in enumerate(avg_topic_probs):
            print(f"  {topics_info[i]['topic_name']}: {ratio:.1%}")
    
    # 6. çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    results = {
        'topics': topics_info,
        'stores': store_analysis,
        'summary': {
            'total_receipts': len(df),
            'total_products': len(feature_names),
            'n_topics': n_topics
        }
    }
    
    with open('analysis_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… åˆ†æå®Œäº†ï¼çµæœã‚’analysis_results.jsonã«ä¿å­˜ã—ã¾ã—ãŸ")
    return results


if __name__ == "__main__":
    import sys
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ãƒˆãƒ”ãƒƒã‚¯æ•°æŒ‡å®šå¯èƒ½
    n_topics = int(sys.argv[1]) if len(sys.argv) > 1 else 6
    print(f"ğŸ¯ {n_topics}ãƒˆãƒ”ãƒƒã‚¯ã§åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™")
    analyze_receipt_data(n_topics)